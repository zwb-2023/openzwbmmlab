{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca32bd6",
   "metadata": {},
   "source": [
    "### openmmlab 训练自己的光谱模型\n",
    "第一步，更改dataset ， 因为原本数据格式不识别hdr，需要增加hdr。倘若是coco格式的标注文件，还需要更改coco标注文件的后缀为hdr。\n",
    "\n",
    "第二步，需要更改继承LoadImageFromFile的方法，更改transform，使其能够读取光谱数据\n",
    "\n",
    "第三步，需要更改数据预处理函数，使其能够处理光谱数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad6d3d",
   "metadata": {},
   "source": [
    "#### 第一步，更改dataset \n",
    "\n",
    "两种更改方式，一种是直接在load_data_list，这个方法中直接添加后缀.hdr或者更改后缀\n",
    "\n",
    "另外一种是增加可识别的后缀，增加hdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 第一种方式\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import copy\n",
    "import traceback\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from typing import List, Union\n",
    "from mmdet.datasets.coco import CocoDataset\n",
    "from mmengine.fileio import get_local_path\n",
    "from my_tools.mmdet.datasets.GXDetDataset import GXDetDataset\n",
    "from mmseg.registry import DATASETS\n",
    "from mmengine.dataset.base_dataset import Compose\n",
    "from mmengine.logging import MMLogger\n",
    "get_root_logger = MMLogger.get_current_instance\n",
    "from my_tools.mmseg.piplines.init_pipeline import LoadCOCOStuffIAnnotations\n",
    "\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class zwbSegDataset(GXDetDataset):\n",
    "\n",
    "    def __init__( self):\n",
    "      \n",
    "\n",
    "    def load_data_list(self):\n",
    "\n",
    "        try:\n",
    "            with get_local_path(\n",
    "                    self.ann_file,\n",
    "                    backend_args=self.backend_args) as local_path:\n",
    "                self.coco = self.COCOAPI(local_path)\n",
    "            # The order of returned `cat_ids` will not\n",
    "            # change with the order of the `classes`\n",
    "            self.cat_ids = self.coco.get_cat_ids(\n",
    "                cat_names=self.metainfo['classes'])\n",
    "            self.cat2label = {\n",
    "                cat_id: i\n",
    "                for i, cat_id in enumerate(self.cat_ids)\n",
    "            }\n",
    "            self.cat_img_map = copy.deepcopy(self.coco.cat_img_map)\n",
    "\n",
    "            img_ids = self.coco.get_img_ids()\n",
    "            data_list = []\n",
    "            total_ann_ids = []\n",
    "            for img_id in img_ids:\n",
    "                raw_img_info = self.coco.load_imgs([img_id])[0]\n",
    "                raw_img_info['img_id'] = img_id\n",
    "\n",
    "                ann_ids = self.coco.get_ann_ids(img_ids=[img_id])\n",
    "                raw_ann_info = self.coco.load_anns(ann_ids)\n",
    "                total_ann_ids.extend(ann_ids)\n",
    "\n",
    "                parsed_data_info = self.parse_data_info({\n",
    "                    'raw_ann_info':\n",
    "                    raw_ann_info,\n",
    "                    'raw_img_info':\n",
    "                    raw_img_info\n",
    "                })\n",
    "                data_list.append(parsed_data_info)\n",
    "            if self.ANN_ID_UNIQUE:\n",
    "                assert len(set(total_ann_ids)) == len(\n",
    "                    total_ann_ids\n",
    "                ), f\"Annotation ids in '{self.ann_file}' are not unique!\"\n",
    "\n",
    "            del self.coco\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for img_data in self.results['json_data_dic']['images']:\n",
    "            if 'file_name' in img_data:\n",
    "                img_data['file_name'] = img_data['file_name'].rsplit('.', 1)[0] + '.hdr'\n",
    "\n",
    "        data_list = [{'img_path': img_data['file_name'].rsplit('.', 1)[0] + '.hdr',} for img_data in self.results['json_data_dic']['images']]\n",
    "\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 第二种方式  在列表上面加上了hdr\n",
    "\n",
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from mmengine import fileio\n",
    "from mmengine.logging import MMLogger\n",
    "\n",
    "from mmpretrain.registry import DATASETS\n",
    "from mmpretrain.datasets import CustomDataset\n",
    "\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class zwbImageNet(CustomDataset):\n",
    "    \"\"\"`ImageNet <http://www.image-net.org>`_ Dataset.\n",
    "\n",
    "    The dataset supports two kinds of directory format,\n",
    "\n",
    "    ::\n",
    "\n",
    "        imagenet\n",
    "        ├── train\n",
    "        │   ├──class_x\n",
    "        |   |   ├── x1.jpg\n",
    "        |   |   ├── x2.jpg\n",
    "        |   |   └── ...\n",
    "        │   ├── class_y\n",
    "        |   |   ├── y1.jpg\n",
    "        |   |   ├── y2.jpg\n",
    "        |   |   └── ...\n",
    "        |   └── ...\n",
    "        ├── val\n",
    "        │   ├──class_x\n",
    "        |   |   └── ...\n",
    "        │   ├── class_y\n",
    "        |   |   └── ...\n",
    "        |   └── ...\n",
    "        └── test\n",
    "            ├── test1.jpg\n",
    "            ├── test2.jpg\n",
    "            └── ...\n",
    "\n",
    "    or ::\n",
    "\n",
    "        imagenet\n",
    "        ├── train\n",
    "        │   ├── x1.jpg\n",
    "        │   ├── y1.jpg\n",
    "        │   └── ...\n",
    "        ├── val\n",
    "        │   ├── x3.jpg\n",
    "        │   ├── y3.jpg\n",
    "        │   └── ...\n",
    "        ├── test\n",
    "        │   ├── test1.jpg\n",
    "        │   ├── test2.jpg\n",
    "        │   └── ...\n",
    "        └── meta\n",
    "            ├── train.txt\n",
    "            └── val.txt\n",
    "\n",
    "\n",
    "    Args:\n",
    "        data_root (str): The root directory for ``data_prefix`` and\n",
    "            ``ann_file``. Defaults to ''.\n",
    "        split (str): The dataset split, supports \"train\", \"val\" and \"test\".\n",
    "            Default to ''.\n",
    "        data_prefix (str | dict): Prefix for training data. Defaults to ''.\n",
    "        ann_file (str): Annotation file path. Defaults to ''.\n",
    "        metainfo (dict, optional): Meta information for dataset, such as class\n",
    "            information. Defaults to None.\n",
    "        **kwargs: Other keyword arguments in :class:`CustomDataset` and\n",
    "            :class:`BaseDataset`.\n",
    "\n",
    "\n",
    "    Examples:\n",
    "        >>> from mmpretrain.datasets import ImageNet\n",
    "        >>> train_dataset = ImageNet(data_root='data/imagenet', split='train')\n",
    "        >>> train_dataset\n",
    "        Dataset ImageNet\n",
    "            Number of samples:  1281167\n",
    "            Number of categories:       1000\n",
    "            Root of dataset:    data/imagenet\n",
    "        >>> test_dataset = ImageNet(data_root='data/imagenet', split='val')\n",
    "        >>> test_dataset\n",
    "        Dataset ImageNet\n",
    "            Number of samples:  50000\n",
    "            Number of categories:       1000\n",
    "            Root of dataset:    data/imagenet\n",
    "    \"\"\"  # noqa: E501\n",
    "\n",
    "    IMG_EXTENSIONS = ('.hdr','.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', 'npy')\n",
    "    # METAINFO = {'classes': IMAGENET_CATEGORIES}\n",
    "    # METAINFO = {'classes': ('0', '1', '2', '3', '4', '5')}\n",
    "    # METAINFO = {'classes': ('0', '1')}\n",
    "    # METAINFO = {'classes': ('1', '4')}\n",
    "    # METAINFO = {'classes': ('fsl', 'jy', 'qsdy', 'rhy', 'sdy')}\n",
    "    # METAINFO = {'classes': ('ljz', 'jnz', 'you')}\n",
    "    # METAINFO = {'classes': ('cjs', 'fsl', 'jy', 'qsdy', 'rhy', 'sdy', 'zyym')}\n",
    "    # METAINFO = {'classes': ('cjs', 'fsl', 'jy', 'qsdy', 'rhy', 'sdy', 'zy')}\n",
    "    # METAINFO = {'classes': ('cjs', 'fsl', 'jnz', 'jy', 'ljz', 'qsdy', 'rhy', 'sdy', 'zy')}\n",
    "    # METAINFO = {'classes': ('jnz', 'ljz', 'no_jnz_ljz')}\n",
    "    # METAINFO = {'classes': ('jnz_ljz', 'no_jnz_ljz')}\n",
    "    # METAINFO = {'classes': ('shui', 'xiangjing', 'you')}\n",
    "    # METAINFO = {'classes': ('fsl_jy_qsdy', 'rhy', 'sdy')}\n",
    "    # METAINFO = {'classes': ('fsl_jy_qsdy', 'rhy', 'sdy', 'zy')}\n",
    "    # METAINFO = {'classes': ('fsl', 'jy', 'qsdy', 'rhy_sdy_zy')}\n",
    "    # METAINFO = {'classes': ('fsl', 'jy', 'qsdy')}\n",
    "    # METAINFO = {'classes': ('cly', 'shl', 'rhy', 'rhz', 'sly', 'zcy')}\n",
    "    METAINFO = {'classes': ('正常', '不正常')}\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_root: str = '',\n",
    "                 split: str = '',\n",
    "                 data_prefix: Union[str, dict] = '',\n",
    "                 ann_file: str = '',\n",
    "                 metainfo: Optional[dict] = None,\n",
    "                 **kwargs):\n",
    "        kwargs = {'extensions': self.IMG_EXTENSIONS, **kwargs}\n",
    "\n",
    "        if split:\n",
    "            splits = ['train', 'val', 'test']\n",
    "            assert split in splits, \\\n",
    "                f\"The split must be one of {splits}, but get '{split}'\"\n",
    "\n",
    "            if split == 'test':\n",
    "                logger = MMLogger.get_current_instance()\n",
    "                logger.info(\n",
    "                    'Since the ImageNet1k test set does not provide label'\n",
    "                    'annotations, `with_label` is set to False')\n",
    "                kwargs['with_label'] = False\n",
    "\n",
    "            data_prefix = split if data_prefix == '' else data_prefix\n",
    "\n",
    "            if ann_file == '':\n",
    "                _ann_path = fileio.join_path(data_root, 'meta', f'{split}.txt')\n",
    "                if fileio.exists(_ann_path):\n",
    "                    ann_file = fileio.join_path('meta', f'{split}.txt')\n",
    "\n",
    "        super().__init__(\n",
    "            data_root=data_root,\n",
    "            data_prefix=data_prefix,\n",
    "            ann_file=ann_file,\n",
    "            metainfo=metainfo,\n",
    "            **kwargs)\n",
    "\n",
    "    def extra_repr(self) -> List[str]:\n",
    "        \"\"\"The extra repr information of the dataset.\"\"\"\n",
    "        body = [\n",
    "            f'Root of dataset: \\t{self.data_root}',\n",
    "        ]\n",
    "        return body\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b281d37",
   "metadata": {},
   "source": [
    "#### 第二步需要更改继承LoadImageFromFile的方法，更改transform，使其能够读取光谱数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mmcv.transforms import LoadImageFromFile\n",
    "import spectral \n",
    "import numpy as np\n",
    "from mmseg.registry import DATASETS, TRANSFORMS\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class LoadSpecralImageFromHdr(LoadImageFromFile):\n",
    "\n",
    "    def transform(self, results: dict):\n",
    "\n",
    "        filename = results['img_path']\n",
    "        img = spectral.open_image(filename)[:,:,200:210]\n",
    "        img = np.clip(img, 0, 255)  # Ensure values are within the valid range\n",
    "        img = img.astype(np.uint8)   # Convert to uint8\n",
    "\n",
    "\n",
    "        if self.to_float32:\n",
    "            img = img.astype(np.float32)\n",
    "\n",
    "        results['img'] = img\n",
    "        results['img_shape'] = img.shape[:2]\n",
    "        results['ori_shape'] = img.shape[:2]\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a86282",
   "metadata": {},
   "source": [
    "#### 第三步，更改预处理方法，使其能够适配光谱图像多出的维度\n",
    "\n",
    "一般预处理方法会有标准化的方法，一般标注化的方法是对RGB三个维度进行标准化，而对光谱图像进行标注化，就要增多相应的维度\n",
    "\n",
    "并且，如有有转换bgr为rgb的操作，或者转换成hsv的操作，一定要找出并且取消掉。对光谱图像处理会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33340832",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 在配置文件中，进行如下更改\n",
    "data_preprocessor = dict(\n",
    "    type='SegDataPreProcessor',\n",
    "    mean=[123.675]*10,   ### 扩增的10是光谱维度\n",
    "    std=[58.395]*10,\n",
    "    bgr_to_rgb=False,  ### 转换取消掉\n",
    "    pad_val=0,\n",
    "    seg_pad_val=255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea50501",
   "metadata": {},
   "source": [
    "#### 第四步更改模型\n",
    "\n",
    "倘若需要优化模型效果，肯定是需要改模型参数的，但是这里只介绍能够跑通的核心，修改输入层数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dict(\n",
    "    type='EncoderDecoder',\n",
    "    data_preprocessor=data_preprocessor,\n",
    "    pretrained='open-mmlab://resnet50_v1c',\n",
    "    backbone=dict(\n",
    "        type='ResNetV1c',\n",
    "        depth=50,\n",
    "        num_stages=4,\n",
    "        out_indices=(0, 1, 2, 3),\n",
    "        dilations=(1, 1, 2, 4),\n",
    "        strides=(1, 2, 1, 1),\n",
    "        norm_cfg=norm_cfg,\n",
    "        norm_eval=False,\n",
    "        style='pytorch',\n",
    "        contract_dilation=True,\n",
    "        in_channels=10)) ### 注意这里in_channels ，10是通道数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a137e48",
   "metadata": {},
   "source": [
    "#### 第五步标注文件中的路径转换（路径转换为hdr为后缀）（coco数据集格式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6662807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully changed path: newdata20250617_145549.png -> newdata20250617_145549.hdr\n",
      "Processed newdata20250617_145549.json\n",
      "Successfully changed path: newdata20250617_145711.png -> newdata20250617_145711.hdr\n",
      "Processed newdata20250617_145711.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path to the folder containing your JSON files\n",
    "folder_path = rf'H:\\\\临时使用数据\\\\高光谱大烟叶数据'\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Only process JSON files\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Open and load the JSON file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Now assuming 'imagePath' is at the first level of the dictionary\n",
    "        if 'imagePath' in data:\n",
    "            original_path = data['imagePath']\n",
    "            try:\n",
    "                # Change the file extension to .hdr\n",
    "                new_path = os.path.splitext(original_path)[0] + '.hdr'\n",
    "                data['imagePath'] = new_path\n",
    "                print(f\"Successfully changed path: {original_path} -> {new_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to change path for {original_path}. Error: {e}\")\n",
    "\n",
    "        # Save the modified JSON back to the file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Processed {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
